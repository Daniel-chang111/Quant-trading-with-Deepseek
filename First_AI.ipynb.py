# ============================================================
# ğŸš€ ì½”ìŠ¤í”¼ 500ê°œ ì¢…ëª© AI í€€íŠ¸ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ
# ============================================================
# ì´ ë…¸íŠ¸ë¶ì€ ì½”ìŠ¤í”¼ ìƒì¥ ì¢…ëª©ë“¤ì˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ ,
# AI(ëœë¤í¬ë ˆìŠ¤íŠ¸)ë¥¼ í™œìš©í•˜ì—¬ íˆ¬ì ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
# 
# ì£¼ìš” ê¸°ëŠ¥:
# 1. ìºì‹œ ì‹œìŠ¤í…œì„ í™œìš©í•œ ê³ ì† ë°ì´í„° ìˆ˜ì§‘
# 2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ (ëœë¤í¬ë ˆìŠ¤íŠ¸)
# 3. íŒ©í„° ì¤‘ìš”ë„ ë¶„ì„
# 4. AI ì¶”ì²œ ì¢…ëª© ì„ ì •
# 5. ê¸°ì¡´ íˆ¬ì ì „ëµê³¼ AI ì „ëµ ë¹„êµ
# ============================================================

# ============================================================
# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸
# ============================================================

# !pip install: ì½”ë©ì—ì„œ íŒŒì´ì¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ëŠ” ëª…ë ¹ì–´
# -q ì˜µì…˜: ì„¤ì¹˜ ê³¼ì •ì„ ì¡°ìš©íˆ(quiet) ì§„í–‰
!pip install pykrx -q

# ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np              # ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë°°ì—´, í–‰ë ¬ ì—°ì‚°)
import pandas as pd             # ë°ì´í„° ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì—‘ì…€ê³¼ ìœ ì‚¬í•œ ë°ì´í„°í”„ë ˆì„)
import matplotlib.pyplot as plt # ê·¸ë˜í”„ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import seaborn as sns           # ë” ì˜ˆìœ ê·¸ë˜í”„ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime, timedelta  # ë‚ ì§œ ê³„ì‚°, í˜•ë³€í™˜ ë“±

# ì‹œìŠ¤í…œ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
import time                      # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •ìš©
import os                         # íŒŒì¼/í´ë” ê²½ë¡œ ì²˜ë¦¬
import pickle                     # íŒŒì´ì¬ ê°ì²´ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° (ìºì‹œìš©)

# í•œêµ­ ì£¼ì‹ ë°ì´í„° ë¼ì´ë¸ŒëŸ¬ë¦¬
from pykrx import stock           # KRX(í•œêµ­ê±°ë˜ì†Œ) ë°ì´í„° ì¡°íšŒ

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (scikit-learn)
from sklearn.ensemble import RandomForestClassifier  # ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸
from sklearn.model_selection import train_test_split # ë°ì´í„°ë¥¼ í•™ìŠµ/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• 
from sklearn.metrics import accuracy_score, confusion_matrix # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
from sklearn.preprocessing import StandardScaler     # ë°ì´í„° ì •ê·œí™” (í‰ê· 0, ë¶„ì‚°1)

print("="*50)
print("âœ… ë¨¸ì‹ ëŸ¬ë‹ í€€íŠ¸ ì‹œì‘!")
print(f"numpy ë²„ì „: {np.__version__}")
print(f"pandas ë²„ì „: {pd.__version__}")
print("="*50)

# ============================================================
# ğŸ“… ë¶„ì„ ê¸°ì¤€ ë‚ ì§œ ì„¤ì •
# ============================================================
# TARGET_DATE: ë¶„ì„í•  ê¸°ì¤€ ë‚ ì§œ (2026ë…„ 2ì›” 13ì¼ ê¸ˆìš”ì¼)
# ì´ ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¶„ì„í•¨
TARGET_DATE = "20260213"

# ============================================================
# 1. ìºì‹œ ì‹œìŠ¤í…œ í´ë˜ìŠ¤
# ============================================================
# ìºì‹œë€? í•œ ë²ˆ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•´ë‘ê³ ,
# ë‹¤ìŒì— ê°™ì€ ë°ì´í„°ê°€ í•„ìš”í•  ë•Œ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” ê¸°ìˆ 
# ì´ë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„ì„ 10ë¶„ â†’ 1ì´ˆë¡œ ë‹¨ì¶•

class StockDataCache:
    """
    ì£¼ì‹ ë°ì´í„° ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤
    - ëª©ì : ë°ì´í„° ìˆ˜ì§‘ ì†ë„ í–¥ìƒ
    - ë°©ë²•: í•œ ë²ˆ ìˆ˜ì§‘í•œ ë°ì´í„°ëŠ” stock_cache í´ë”ì— pkl íŒŒì¼ë¡œ ì €ì¥
    - ë‹¤ìŒì— ê°™ì€ ë°ì´í„° ìš”ì²­ ì‹œ íŒŒì¼ì—ì„œ ë°”ë¡œ ë¶ˆëŸ¬ì˜´
    """
    
    def __init__(self, cache_dir='stock_cache'):
        """
        ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        Args:
            cache_dir: ìºì‹œ íŒŒì¼ì„ ì €ì¥í•  í´ë” ì´ë¦„ (ê¸°ë³¸: 'stock_cache')
        """
        self.cache_dir = cache_dir
        # í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)

    def get_cache_path(self, ticker, date):
        """
        ìºì‹œ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ ìƒì„±
        Args:
            ticker: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
            date: ê¸°ì¤€ ë‚ ì§œ (ì˜ˆ: '20260213')
        Returns:
            ìºì‹œ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: 'stock_cache/005930_20260213.pkl')
        """
        return f"{self.cache_dir}/{ticker}_{date}.pkl"

    def save(self, ticker, date, data):
        """
        ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥
        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            date: ê¸°ì¤€ ë‚ ì§œ
            data: ì €ì¥í•  ë°ì´í„° (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        """
        # íŒŒì¼ì„ ì“°ê¸°(w) ë°”ì´ë„ˆë¦¬(b) ëª¨ë“œë¡œ ì—´ê¸°
        with open(self.get_cache_path(ticker, date), 'wb') as f:
            # pickle.dump: íŒŒì´ì¬ ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            pickle.dump(data, f)

    def load(self, ticker, date):
        """
        ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ
        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            date: ê¸°ì¤€ ë‚ ì§œ
        Returns:
            ì €ì¥ëœ ë°ì´í„° (ìˆìœ¼ë©´), None (ì—†ìœ¼ë©´)
        """
        path = self.get_cache_path(ticker, date)
        # os.path.exists: íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if os.path.exists(path):
            # íŒŒì¼ì„ ì½ê¸°(r) ë°”ì´ë„ˆë¦¬(b) ëª¨ë“œë¡œ ì—´ê¸°
            with open(path, 'rb') as f:
                # pickle.load: íŒŒì¼ì—ì„œ íŒŒì´ì¬ ê°ì²´ ë¡œë“œ
                return pickle.load(f)
        return None

# ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì „ì—­ì—ì„œ ì‚¬ìš©)
cache = StockDataCache()


# ============================================================
# 2. ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def fetch_ticker_data(ticker, date):
    """
    í•˜ë‚˜ì˜ ì¢…ëª©ì— ëŒ€í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    - ìºì‹œ í™•ì¸ -> ìˆìœ¼ë©´ ìºì‹œ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìˆ˜ì§‘
    - ìˆ˜ì§‘ í•­ëª©: PER, PBR, ë°°ë‹¹ìˆ˜ìµë¥ , ì‹œê°€ì´ì•¡, ê±°ë˜ëŒ€ê¸ˆ,
                ëª¨ë©˜í…€(1,3,6ê°œì›”), ë³€ë™ì„±, í˜„ì¬ê°€
    
    Args:
        ticker: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
        date: ê¸°ì¤€ ë‚ ì§œ (ì˜ˆ: '20260213')
    
    Returns:
        result ë”•ì…”ë„ˆë¦¬ (ì„±ê³µ ì‹œ), None (ì‹¤íŒ¨ ì‹œ)
    """
    
    # 1. ìºì‹œ í™•ì¸
    cached = cache.load(ticker, date)
    if cached is not None:  # ìºì‹œì— ë°ì´í„°ê°€ ìˆìœ¼ë©´
        return cached       # ë°”ë¡œ ë°˜í™˜ (ì‹œê°„ ì ˆì•½!)

    try:
        # 2. ì¢…ëª©ëª… ì¡°íšŒ (ì˜ˆ: '005930' -> 'ì‚¼ì„±ì „ì')
        name = stock.get_market_ticker_name(ticker)

        # 3. PER, PBR, ë°°ë‹¹ìˆ˜ìµë¥  ë°ì´í„° ì¡°íšŒ
        # get_market_fundamental: ì¬ë¬´ì œí‘œ ê¸°ë°˜ ë°ì´í„° ì¡°íšŒ
        df_fund = stock.get_market_fundamental(date, date, ticker)
        if df_fund.empty:  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨
            return None

        # iloc[0]: ì²« ë²ˆì§¸ í–‰(í•´ë‹¹ ë‚ ì§œ ë°ì´í„°) ê°€ì ¸ì˜¤ê¸°
        fund_row = df_fund.iloc[0]
        
        # ê° ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ê°’ ì¶”ì¶œ (ì—†ìœ¼ë©´ NaN)
        # index: ë°ì´í„°í”„ë ˆì„ì˜ ì»¬ëŸ¼ëª… ëª©ë¡
        per = fund_row['PER'] if 'PER' in fund_row.index else np.nan
        pbr = fund_row['PBR'] if 'PBR' in fund_row.index else np.nan
        div = fund_row['DIV'] if 'DIV' in fund_row.index else 0

        # 4. ì‹œê°€ì´ì•¡, ê±°ë˜ëŒ€ê¸ˆ ë°ì´í„° ì¡°íšŒ
        df_cap = stock.get_market_cap(date, date, ticker)
        if df_cap.empty:
            return None

        cap_row = df_cap.iloc[0]
        market_cap = cap_row['ì‹œê°€ì´ì•¡'] if 'ì‹œê°€ì´ì•¡' in cap_row.index else np.nan
        volume = cap_row['ê±°ë˜ëŒ€ê¸ˆ'] if 'ê±°ë˜ëŒ€ê¸ˆ' in cap_row.index else np.nan

        # 5. PER, PBR ìœ íš¨ì„± ê²€ì‚¬
        # pd.isna: NaN(ê²°ì¸¡ì¹˜) í™•ì¸
        if pd.isna(per) or pd.isna(pbr) or per <= 0 or pbr <= 0:
            return None  # ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°ëŠ” ì œì™¸

        # 6. ë¡œê·¸ ë³€í™˜ (ì‹œê°€ì´ì•¡ì´ 1ì¡°, 100ì¡° ë“± ì°¨ì´ê°€ ì»¤ì„œ ë¡œê·¸ ì·¨í•˜ë©´ ì •ê·œë¶„í¬ì— ê°€ê¹Œì›Œì§)
        market_cap_log = np.log(market_cap) if market_cap > 0 else np.nan
        volume_log = np.log(volume) if volume > 0 else np.nan

        # 7. ê°€ê²© ë°ì´í„° ì¡°íšŒ (ëª¨ë©˜í…€, ë³€ë™ì„± ê³„ì‚°ìš©)
        # 200ì¼ ì „ ë‚ ì§œ ê³„ì‚° (timedelta: ë‚ ì§œ ì°¨ì´)
        start_date = (datetime.strptime(date, '%Y%m%d') - timedelta(days=200)).strftime('%Y%m%d')
        # get_market_ohlcv_by_date: ì¼ë³„ OHLCV ë°ì´í„° ì¡°íšŒ (Open, High, Low, Close, Volume)
        df_price = stock.get_market_ohlcv_by_date(start_date, date, ticker)

        if df_price is None or len(df_price) < 100:
            return None  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì œì™¸

        # 8. ëª¨ë©˜í…€ ê³„ì‚°
        # ëª¨ë©˜í…€ = (í˜„ì¬ê°€ / ê³¼ê±°ê°€ - 1) * 100 (ë°±ë¶„ìœ¨)
        # iloc[-1]: ë§ˆì§€ë§‰ í–‰(ìµœì‹  ë°ì´í„°), iloc[-22]: 22ì¼ ì „(ì•½ 1ê°œì›”)
        momentum_1m = (df_price['ì¢…ê°€'].iloc[-1] / df_price['ì¢…ê°€'].iloc[-22] - 1) * 100 if len(df_price) > 22 else np.nan
        momentum_3m = (df_price['ì¢…ê°€'].iloc[-1] / df_price['ì¢…ê°€'].iloc[-66] - 1) * 100 if len(df_price) > 66 else np.nan
        momentum_6m = (df_price['ì¢…ê°€'].iloc[-1] / df_price['ì¢…ê°€'].iloc[-132] - 1) * 100 if len(df_price) > 132 else np.nan
        
        # 9. ë³€ë™ì„± ê³„ì‚°
        # tail(60): ìµœê·¼ 60ì¼ ë°ì´í„°, std(): í‘œì¤€í¸ì°¨
        volatility = df_price['ë“±ë½ë¥ '].tail(60).std() if len(df_price) >= 60 else np.nan

        # 10. target (ì„ì‹œ) - ì‹¤ì œë¡œëŠ” ë‹¤ìŒë‹¬ ìˆ˜ìµë¥ ë¡œ ëŒ€ì²´í•´ì•¼ í•¨
        # np.random.random(): 0~1 ì‚¬ì´ ëœë¤ê°’, 0.5ë³´ë‹¤ í¬ë©´ 1(ìƒìŠ¹), ì‘ìœ¼ë©´ 0(í•˜ë½)
        target = 1 if np.random.random() > 0.5 else 0

        # 11. ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        result = {
            'í‹°ì»¤': ticker,
            'ì¢…ëª©': name,
            'PER': round(per, 2),
            'PBR': round(pbr, 2),
            'ë°°ë‹¹ìˆ˜ìµë¥ ': round(div, 2),
            'ì‹œê°€ì´ì•¡': round(market_cap_log, 2) if not np.isnan(market_cap_log) else np.nan,
            'ê±°ë˜ëŒ€ê¸ˆ': round(volume_log, 2) if not np.isnan(volume_log) else np.nan,
            'ìˆ˜ìµë¥ _1ê°œì›”': round(momentum_1m, 2) if not np.isnan(momentum_1m) else np.nan,
            'ìˆ˜ìµë¥ _3ê°œì›”': round(momentum_3m, 2) if not np.isnan(momentum_3m) else np.nan,
            'ìˆ˜ìµë¥ _6ê°œì›”': round(momentum_6m, 2) if not np.isnan(momentum_6m) else np.nan,
            'ë³€ë™ì„±': round(volatility, 2) if not np.isnan(volatility) else np.nan,
            'target': target
        }

        # 12. ìºì‹œ ì €ì¥ (ë‹¤ìŒì— ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´)
        cache.save(ticker, date, result)
        return result

    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ None ë°˜í™˜ (í•´ë‹¹ ì¢…ëª©ì€ ê±´ë„ˆëœ€)
        return None


# ============================================================
# 3. ì›”ë³„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬)
# ============================================================
# ThreadPoolExecutor: ì—¬ëŸ¬ ì‘ì—…ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from concurrent.futures import ThreadPoolExecutor, as_completed

def collect_month_data_parallel(date, max_workers=10):
    """
    íŠ¹ì • ì›”ì˜ ëª¨ë“  ì¢…ëª© ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘
    Args:
        date: ê¸°ì¤€ ë‚ ì§œ
        max_workers: ë™ì‹œì— ì²˜ë¦¬í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ 10)
    Returns:
        í•´ë‹¹ ì›”ì˜ ì¢…ëª© ë°ì´í„°í”„ë ˆì„
    """
    
    print(f"ğŸ“… {date} ìˆ˜ì§‘ì¤‘...")

    # ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ (ì½”ìŠ¤í”¼ ì „ì²´ì—ì„œ 300ê°œë§Œ)
    tickers = stock.get_market_ticker_list(date, market="KOSPI")[:300]

    results = []
    
    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ì¢…ëª©ë³„ë¡œ fetch_ticker_data í•¨ìˆ˜ ì‹¤í–‰ ì˜ˆì•½
        # executor.submit(í•¨ìˆ˜, ì¸ì1, ì¸ì2) -> Future ê°ì²´ ë°˜í™˜
        futures = {executor.submit(fetch_ticker_data, ticker, date): ticker
                  for ticker in tickers}

        # as_completed: ì‘ì—…ì´ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ë°˜í™˜
        for future in as_completed(futures):
            result = future.result()
            if result:
                result['ê¸°ì¤€ì¼'] = date  # ê¸°ì¤€ì¼ ì»¬ëŸ¼ ì¶”ê°€
                results.append(result)

    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(results)
    print(f"  âœ… {date}: {len(df)}ê°œ ì¢…ëª©")
    return df


# ============================================================
# 4. 2025ë…„ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ë³‘ë ¬ + ìºì‹œ)
# ============================================================
def collect_2025_complete():
    """
    2025ë…„ 1ì›”~12ì›” ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
    - ìºì‹œê°€ ìˆìœ¼ë©´ 1ì´ˆ, ì—†ìœ¼ë©´ 3-5ë¶„ ì†Œìš”
    Returns:
        2025ë…„ ì „ì²´ ì¢…ëª© ë°ì´í„°í”„ë ˆì„
    """

    # ì „ì²´ ìºì‹œ íŒŒì¼ í™•ì¸
    cache_file = '2025_complete_cache.pkl'
    if os.path.exists(cache_file):
        print("ğŸ“¦ ì „ì²´ ìºì‹œ ë°œê²¬! ì¦‰ì‹œ ë¡œë”©...")
        with open(cache_file, 'rb') as f:
            return pickle.load(f)

    # 2025ë…„ ì›”ë³„ ë§ˆì§€ë§‰ ê±°ë˜ì¼ ëª©ë¡
    months = [
        '20250131', '20250228', '20250331', '20250430',
        '20250530', '20250630', '20250731', '20250829',
        '20250930', '20251031', '20251128', '20251230'
    ]

    all_data = []
    # ê° ì›”ë³„ ë°ì´í„° ìˆ˜ì§‘
    for month in months:
        df_month = collect_month_data_parallel(month, max_workers=10)
        if len(df_month) > 0:
            all_data.append(df_month)

    # ëª¨ë“  ì›” ë°ì´í„° í•©ì¹˜ê¸°
    final_df = pd.concat(all_data, ignore_index=True)

    # ì „ì²´ ìºì‹œ ì €ì¥
    with open(cache_file, 'wb') as f:
        pickle.dump(final_df, f)

    return final_df


# ============================================================
# 5. ë‹¤ìŒë‹¬ ìˆ˜ìµë¥  ì¶”ê°€ í•¨ìˆ˜ (ì§„ì§œ target!)
# ============================================================
def add_future_returns(df):
    """
    ê° ì¢…ëª©ì˜ ë‹¤ìŒë‹¬ ìˆ˜ìµë¥  ê³„ì‚°
    - target: ë‹¤ìŒë‹¬ ìˆ˜ìµë¥ ì´ ì–‘ìˆ˜ë©´ 1, ìŒìˆ˜ë©´ 0
    Args:
        df: ì¢…ëª©ë³„ ì›”ë³„ ë°ì´í„°
    Returns:
        targetì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """

    results = []
    # ì¢…ëª©ë³„ë¡œ ê·¸ë£¹í™”
    grouped = df.groupby('í‹°ì»¤')

    for ticker, group in grouped:
        # ê¸°ì¤€ì¼ ìˆœìœ¼ë¡œ ì •ë ¬
        group = group.sort_values('ê¸°ì¤€ì¼')

        # ë‹¤ìŒë‹¬ ìˆ˜ìµë¥  ê³„ì‚° (í˜„ì¬ì›”ê³¼ ë‹¤ìŒì›” ë°ì´í„° í•„ìš”)
        for i in range(len(group)-1):
            current = group.iloc[i]
            next_row = group.iloc[i+1]

            # ë‹¤ìŒë‹¬ ìˆ˜ìµë¥  = (ë‹¤ìŒë‹¬ ì¢…ê°€ / í˜„ì¬ë‹¬ ì¢…ê°€ - 1) * 100
            future_return = (next_row['í˜„ì¬ê°€'] / current['í˜„ì¬ê°€'] - 1) * 100

            row_dict = current.to_dict()
            row_dict['ë‹¤ìŒë‹¬ìˆ˜ìµë¥ '] = round(future_return, 2)
            # target: ì–‘ìˆ˜ë©´ 1(ìƒìŠ¹), ìŒìˆ˜ë©´ 0(í•˜ë½)
            row_dict['target'] = 1 if future_return > 0 else 0

            results.append(row_dict)

    return pd.DataFrame(results)


# ============================================================
# 6. íŒ©í„° ì¤‘ìš”ë„ ë¶„ì„ í•¨ìˆ˜
# ============================================================
def analyze_feature_importance(model, feature_cols):
    """
    ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì˜ íŒ©í„° ì¤‘ìš”ë„ ë¶„ì„
    - feature_importances_: ê° íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ (0~1)
    Args:
        model: í•™ìŠµëœ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸
        feature_cols: íŠ¹ì„±(íŒ©í„°) ì´ë¦„ ëª©ë¡
    Returns:
        ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    
    # ëª¨ë¸ì—ì„œ ì¤‘ìš”ë„ ì¶”ì¶œ
    importances = model.feature_importances_
    # ì¤‘ìš”ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•œ ì¸ë±ìŠ¤
    indices = np.argsort(importances)[::-1]

    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬
    importance_df = pd.DataFrame({
        'íŒ©í„°': [feature_cols[i] for i in indices],
        'ì¤‘ìš”ë„': [importances[i] for i in indices]
    })

    print("\n" + "="*60)
    print("ğŸ“Š íŒ©í„° ì¤‘ìš”ë„ ë¶„ì„")
    print("="*60)
    print(importance_df.to_string(index=False))

    # ì‹œê°í™”
    plt.figure(figsize=(12, 6))
    colors = plt.cm.viridis(np.linspace(0, 1, len(feature_cols)))
    plt.bar(range(len(feature_cols)), importances[indices], color=colors)
    plt.xticks(range(len(feature_cols)), [feature_cols[i] for i in indices], rotation=45)
    plt.title('íŒ©í„° ì¤‘ìš”ë„ (ë†’ì„ìˆ˜ë¡ ì˜í–¥ë ¥ í¼)')
    plt.xlabel('íŒ©í„°')
    plt.ylabel('ì¤‘ìš”ë„')
    plt.tight_layout()
    plt.show()

    # ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
    print("\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:")
    print(f"  ê°€ì¥ ì¤‘ìš”í•œ íŒ©í„° 3ê°œ: {', '.join([feature_cols[indices[i]] for i in range(3)])}")
    print(f"  ìƒìœ„ 3ê°œ íŒ©í„°ì˜ ëˆ„ì  ì¤‘ìš”ë„: {sum(importances[indices][:3]):.2%}")

    return importance_df


# ============================================================
# 7. AI ì¶”ì²œ ì¢…ëª© ì„ ì • í•¨ìˆ˜
# ============================================================
def get_ai_recommendations(df, model, scaler, feature_cols, top_n=20):
    """
    AI ëª¨ë¸ë¡œ ì¶”ì²œ ì¢…ëª© ì„ ì •
    Args:
        df: ì¢…ëª© ë°ì´í„°
        model: í•™ìŠµëœ ëª¨ë¸
        scaler: ì •ê·œí™” ê°ì²´
        feature_cols: íŠ¹ì„± ëª©ë¡
        top_n: ì¶”ì²œí•  ì¢…ëª© ìˆ˜
    Returns:
        ìƒìŠ¹í™•ë¥  ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    
    df_result = df.copy()
    
    # ë°ì´í„° ì •ê·œí™”
    X_all = scaler.transform(df[feature_cols].values)
    
    # ìƒìŠ¹ í™•ë¥  ì˜ˆì¸¡ (predict_proba: ê° í´ë˜ìŠ¤ í™•ë¥  ë°˜í™˜)
    # [:, 1] : í´ë˜ìŠ¤ 1(ìƒìŠ¹)ì˜ í™•ë¥ ë§Œ ì„ íƒ
    df_result['ìƒìŠ¹í™•ë¥ '] = model.predict_proba(X_all)[:, 1]

    # ì¶”ì²œ ë“±ê¸‰ ë¶€ì—¬ (ìƒìœ„ 20% = ë§¤ìˆ˜, ì¤‘ê°„ 60% = ê´€ì‹¬, í•˜ìœ„ 20% = ê´€ë§)
    # pd.qcut: ë°ì´í„°ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ”
    df_result['ì¶”ì²œë“±ê¸‰'] = pd.qcut(df_result['ìƒìŠ¹í™•ë¥ '], 
                                     q=[0, 0.2, 0.8, 1.0],
                                     labels=['ê´€ë§', 'ê´€ì‹¬', 'ë§¤ìˆ˜'])

    print("\n" + "="*60)
    print(f"ğŸ¤– AI ì¶”ì²œ TOP {top_n} ì¢…ëª©")
    print("="*60)

    # ìƒìŠ¹í™•ë¥  ë†’ì€ ìˆœ ì •ë ¬
    top_stocks = df_result.sort_values('ìƒìŠ¹í™•ë¥ ', ascending=False).head(top_n)

    # ì¶œë ¥í•  ì»¬ëŸ¼ ì„ íƒ
    display_cols = ['ì¢…ëª©', 'PER', 'PBR', 'ë°°ë‹¹ìˆ˜ìµë¥ ',
                    'ìˆ˜ìµë¥ _1ê°œì›”', 'ìˆ˜ìµë¥ _3ê°œì›”', 'ë³€ë™ì„±', 'ìƒìŠ¹í™•ë¥ ', 'ì¶”ì²œë“±ê¸‰']

    print(top_stocks[display_cols].to_string(index=False))

    return top_stocks


# ============================================================
# 8. ê¸°ì¡´ ì „ëµ vs AI ì „ëµ ë¹„êµ í•¨ìˆ˜
# ============================================================
def compare_strategies(df, model, scaler, feature_cols, top_n=10):
    """
    ê¸°ì¡´ íˆ¬ì ì „ëµê³¼ AI ì „ëµì„ ë¹„êµ ë¶„ì„
    ë¹„êµ ì „ëµ:
    1. ê°€ì¹˜ì£¼ ì „ëµ (ì €PER + ì €PBR)
    2. ëª¨ë©˜í…€ ì „ëµ (ìµœê·¼ ìˆ˜ìµë¥  ë†’ì€ ìˆœ)
    3. ì €ë³€ë™ì„± ì „ëµ (ë³€ë™ì„± ë‚®ì€ ìˆœ)
    4. AI ì „ëµ (ìƒìŠ¹í™•ë¥  ë†’ì€ ìˆœ)
    """
    
    df_strat = df.copy()

    # === 1. AI ì ìˆ˜ ê³„ì‚° ===
    X_all = scaler.transform(df[feature_cols].values)
    df_strat['AIì ìˆ˜'] = model.predict_proba(X_all)[:, 1]

    # === 2. ê°€ì¹˜ì£¼ ì „ëµ ===
    # PERê³¼ PBRì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ìŒìˆ˜ ê°€ì¤‘ì¹˜)
    df_strat['ê°€ì¹˜ì ìˆ˜'] = -df_strat['PER'] - df_strat['PBR']

    # === 3. ëª¨ë©˜í…€ ì „ëµ ===
    df_strat['ëª¨ë©˜í…€ì ìˆ˜'] = df_strat['ìˆ˜ìµë¥ _1ê°œì›”'] + df_strat['ìˆ˜ìµë¥ _3ê°œì›”']

    # === 4. ì €ë³€ë™ì„± ì „ëµ ===
    df_strat['ì €ë³€ë™ì„±ì ìˆ˜'] = -df_strat['ë³€ë™ì„±']

    # === 5. ê° ì „ëµë³„ TOP 10 ì„ ì • ===
    value_top = df_strat.nlargest(top_n, 'ê°€ì¹˜ì ìˆ˜')[['ì¢…ëª©', 'PER', 'PBR', 'ê°€ì¹˜ì ìˆ˜']].copy()
    value_top['ì „ëµ'] = 'ê°€ì¹˜ì£¼'

    momentum_top = df_strat.nlargest(top_n, 'ëª¨ë©˜í…€ì ìˆ˜')[['ì¢…ëª©', 'PER', 'PBR', 'ëª¨ë©˜í…€ì ìˆ˜']].copy()
    momentum_top['ì „ëµ'] = 'ëª¨ë©˜í…€'

    lowvol_top = df_strat.nlargest(top_n, 'ì €ë³€ë™ì„±ì ìˆ˜')[['ì¢…ëª©', 'PER', 'PBR', 'ì €ë³€ë™ì„±ì ìˆ˜']].copy()
    lowvol_top['ì „ëµ'] = 'ì €ë³€ë™ì„±'

    ai_top = df_strat.nlargest(top_n, 'AIì ìˆ˜')[['ì¢…ëª©', 'PER', 'PBR', 'AIì ìˆ˜']].copy()
    ai_top['ì „ëµ'] = 'AI'

    # === 6. ê²°ê³¼ ì¶œë ¥ ===
    print("\n" + "="*70)
    print("ğŸ“Š ì „ëµë³„ TOP 10 ë¹„êµ")
    print("="*70)

    print("\nğŸ† ê°€ì¹˜ì£¼ ì „ëµ TOP 10 (ì €PER + ì €PBR):")
    print(value_top.to_string(index=False))

    print("\nğŸš€ ëª¨ë©˜í…€ ì „ëµ TOP 10 (ìµœê·¼ ìˆ˜ìµë¥  ë†’ì€ ìˆœ):")
    print(momentum_top.to_string(index=False))

    print("\nğŸ›¡ï¸ ì €ë³€ë™ì„± ì „ëµ TOP 10 (ë³€ë™ì„± ë‚®ì€ ìˆœ):")
    print(lowvol_top.to_string(index=False))

    print("\nğŸ¤– AI ì „ëµ TOP 10 (ìƒìŠ¹í™•ë¥  ë†’ì€ ìˆœ):")
    print(ai_top.to_string(index=False))

    # === 7. ì¤‘ë³µ ì¢…ëª© ë¶„ì„ ===
    value_set = set(value_top['ì¢…ëª©'].head(5))
    momentum_set = set(momentum_top['ì¢…ëª©'].head(5))
    lowvol_set = set(lowvol_top['ì¢…ëª©'].head(5))
    ai_set = set(ai_top['ì¢…ëª©'].head(5))

    print("\n" + "="*70)
    print("ğŸ” ì „ëµë³„ ì¤‘ë³µ ì¢…ëª© ë¶„ì„ (TOP 5 ê¸°ì¤€)")
    print("="*70)
    print(f"  ê°€ì¹˜ì£¼ âˆ© AI: {value_set & ai_set}")
    print(f"  ëª¨ë©˜í…€ âˆ© AI: {momentum_set & ai_set}")
    print(f"  ì €ë³€ë™ì„± âˆ© AI: {lowvol_set & ai_set}")
    print(f"  ê°€ì¹˜ì£¼ âˆ© ëª¨ë©˜í…€: {value_set & momentum_set}")
    print(f"  ëª¨ë“  ì „ëµ ê³µí†µ: {value_set & momentum_set & lowvol_set & ai_set}")

    # === 8. ì „ëµë³„ íŠ¹ì„± ë¹„êµ ===
    print("\n" + "="*70)
    print("ğŸ“ˆ ì „ëµë³„ í¬íŠ¸í´ë¦¬ì˜¤ íŠ¹ì„± ë¹„êµ")
    print("="*70)

    comparison = pd.DataFrame({
        'ì „ëµ': ['ê°€ì¹˜ì£¼', 'ëª¨ë©˜í…€', 'ì €ë³€ë™ì„±', 'AI'],
        'í‰ê·  PER': [
            value_top['PER'].mean(),
            momentum_top['PER'].mean(),
            lowvol_top['PER'].mean(),
            ai_top['PER'].mean()
        ],
        'í‰ê·  PBR': [
            value_top['PBR'].mean(),
            momentum_top['PBR'].mean(),
            lowvol_top['PBR'].mean(),
            ai_top['PBR'].mean()
        ],
        'í‰ê·  ìˆ˜ìµë¥ _1ê°œì›”': [
            df_strat[df_strat['ì¢…ëª©'].isin(value_top['ì¢…ëª©'])]['ìˆ˜ìµë¥ _1ê°œì›”'].mean(),
            df_strat[df_strat['ì¢…ëª©'].isin(momentum_top['ì¢…ëª©'])]['ìˆ˜ìµë¥ _1ê°œì›”'].mean(),
            df_strat[df_strat['ì¢…ëª©'].isin(lowvol_top['ì¢…ëª©'])]['ìˆ˜ìµë¥ _1ê°œì›”'].mean(),
            df_strat[df_strat['ì¢…ëª©'].isin(ai_top['ì¢…ëª©'])]['ìˆ˜ìµë¥ _1ê°œì›”'].mean()
        ],
        'í‰ê·  ë³€ë™ì„±': [
            df_strat[df_strat['ì¢…ëª©'].isin(value_top['ì¢…ëª©'])]['ë³€ë™ì„±'].mean(),
            df_strat[df_strat['ì¢…ëª©'].isin(momentum_top['ì¢…ëª©'])]['ë³€ë™ì„±'].mean(),
            df_strat[df_strat['ì¢…ëª©'].isin(lowvol_top['ì¢…ëª©'])]['ë³€ë™ì„±'].mean(),
            df_strat[df_strat['ì¢…ëª©'].isin(ai_top['ì¢…ëª©'])]['ë³€ë™ì„±'].mean()
        ]
    })
    print(comparison.round(2).to_string(index=False))

    # === 9. ì‹œê°í™” ===
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # ê°€ì¹˜ì£¼ ì „ëµ ì‹œê°í™”
    axes[0,0].scatter(df_strat['PER'], df_strat['PBR'], alpha=0.3, s=10, label='ì „ì²´ì¢…ëª©')
    axes[0,0].scatter(value_top['PER'], value_top['PBR'], color='red', s=100, label='ê°€ì¹˜ì£¼ TOP10', alpha=0.7)
    axes[0,0].set_xlabel('PER')
    axes[0,0].set_ylabel('PBR')
    axes[0,0].set_title('ê°€ì¹˜ì£¼ ì „ëµ')
    axes[0,0].legend()
    axes[0,0].set_xlim(0, 50)
    axes[0,0].set_ylim(0, 5)
    axes[0,0].grid(True, alpha=0.3)

    # ëª¨ë©˜í…€ ì „ëµ ì‹œê°í™”
    axes[0,1].scatter(df_strat['PER'], df_strat['PBR'], alpha=0.3, s=10, label='ì „ì²´ì¢…ëª©')
    axes[0,1].scatter(momentum_top['PER'], momentum_top['PBR'], color='blue', s=100, label='ëª¨ë©˜í…€ TOP10', alpha=0.7)
    axes[0,1].set_xlabel('PER')
    axes[0,1].set_ylabel('PBR')
    axes[0,1].set_title('ëª¨ë©˜í…€ ì „ëµ')
    axes[0,1].legend()
    axes[0,1].set_xlim(0, 50)
    axes[0,1].set_ylim(0, 5)
    axes[0,1].grid(True, alpha=0.3)

    # ì €ë³€ë™ì„± ì „ëµ ì‹œê°í™”
    axes[1,0].scatter(df_strat['PER'], df_strat['PBR'], alpha=0.3, s=10, label='ì „ì²´ì¢…ëª©')
    axes[1,0].scatter(lowvol_top['PER'], lowvol_top['PBR'], color='green', s=100, label='ì €ë³€ë™ì„± TOP10', alpha=0.7)
    axes[1,0].set_xlabel('PER')
    axes[1,0].set_ylabel('PBR')
    axes[1,0].set_title('ì €ë³€ë™ì„± ì „ëµ')
    axes[1,0].legend()
    axes[1,0].set_xlim(0, 50)
    axes[1,0].set_ylim(0, 5)
    axes[1,0].grid(True, alpha=0.3)

    # AI ì „ëµ ì‹œê°í™”
    axes[1,1].scatter(df_strat['PER'], df_strat['PBR'], alpha=0.3, s=10, label='ì „ì²´ì¢…ëª©')
    axes[1,1].scatter(ai_top['PER'], ai_top['PBR'], color='purple', s=100, label='AI TOP10', alpha=0.7)
    axes[1,1].set_xlabel('PER')
    axes[1,1].set_ylabel('PBR')
    axes[1,1].set_title('AI ì „ëµ')
    axes[1,1].legend()
    axes[1,1].set_xlim(0, 50)
    axes[1,1].set_ylim(0, 5)
    axes[1,1].grid(True, alpha=0.3)

    plt.suptitle('ì „ëµë³„ TOP 10 ì¢…ëª© ë¶„í¬ ë¹„êµ', fontsize=16)
    plt.tight_layout()
    plt.show()

    # === 10. ì „ëµë³„ ì„±ê³¼ ì˜ˆì¸¡ ===
    print("\n" + "="*70)
    print("ğŸ“Š ì „ëµë³„ ì˜ˆìƒ ì„±ê³¼ ë¹„êµ")
    print("="*70)

    # ê° ì „ëµì˜ í‰ê·  AI ì ìˆ˜ ê³„ì‚°
    value_ai_score = df_strat[df_strat['ì¢…ëª©'].isin(value_top['ì¢…ëª©'])]['AIì ìˆ˜'].mean()
    momentum_ai_score = df_strat[df_strat['ì¢…ëª©'].isin(momentum_top['ì¢…ëª©'])]['AIì ìˆ˜'].mean()
    lowvol_ai_score = df_strat[df_strat['ì¢…ëª©'].isin(lowvol_top['ì¢…ëª©'])]['AIì ìˆ˜'].mean()
    ai_ai_score = df_strat[df_strat['ì¢…ëª©'].isin(ai_top['ì¢…ëª©'])]['AIì ìˆ˜'].mean()

    performance = pd.DataFrame({
        'ì „ëµ': ['ê°€ì¹˜ì£¼', 'ëª¨ë©˜í…€', 'ì €ë³€ë™ì„±', 'AI'],
        'AI í‰ê·  ì ìˆ˜': [
            f"{value_ai_score:.1%}",
            f"{momentum_ai_score:.1%}",
            f"{lowvol_ai_score:.1%}",
            f"{ai_ai_score:.1%}"
        ]
    })
    print(performance.to_string(index=False))

    print("\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:")
    if ai_ai_score > max(value_ai_score, momentum_ai_score, lowvol_ai_score):
        print("  âœ… AI ì „ëµì´ ë‹¤ë¥¸ ì „ëµë³´ë‹¤ ìš°ìˆ˜í•œ ì¢…ëª©ì„ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("  ğŸ¤” ê¸°ì¡´ ì „ëµê³¼ AI ì „ëµì´ ë¹„ìŠ·í•œ ì„±ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.")

    return value_top, momentum_top, lowvol_top, ai_top


# ============================================================
# ì‹¤í–‰! (force_refresh=Falseë¡œ ìºì‹œ ì‚¬ìš©)
# ============================================================
print("="*60)
print("ğŸš€ 2026ë…„ 2ì›” 13ì¼ ê¸ˆìš”ì¼ ê¸°ì¤€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ ìµœì í™”)")
print("="*60)

# ğŸ”¥ ì¤‘ìš”: force_refresh=Falseë¡œ ì„¤ì •í•´ì„œ ìºì‹œ ì‚¬ìš©!
df = get_ml_data_final(
    n_stocks=500,
    use_cache=True,
    force_refresh=False  # Falseë©´ ë¬´ì¡°ê±´ ìºì‹œ ì‚¬ìš©!
)

if len(df) > 0:
    print("\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° ì •ë³´:")
    print(f"ì¢…ëª© ìˆ˜: {len(df)}ê°œ")
    print(f"íŒ©í„° ëª©ë¡: {df.columns.tolist()}")

    print("\nğŸ“ˆ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 10ê°œ):")
    print(df.head(10))

    print("\nğŸ“Š íŒ©í„°ë³„ í†µê³„:")
    print(df.describe())

    # ============================================================
    # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
    # ============================================================
    print("\n" + "="*60)
    print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*60)

    # íŒ©í„° ì»¬ëŸ¼ (ì¢…ëª©, target ì œì™¸)
    feature_cols = [col for col in df.columns if col not in ['ì¢…ëª©', 'target']]
    X = df[feature_cols].values
    y = df['target'].values

    # ë°ì´í„° ì •ê·œí™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )

    # ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸
    model = RandomForestClassifier(